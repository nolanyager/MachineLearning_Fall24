{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2\n",
    "\n",
    "Due: 2022-10-16 at 8:30 AM PT\n",
    "\n",
    "# Name: Nolan Yager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, you will practice downloadings, cleaning, and analyzing data from the [National Risk Index (NRI)](https://hazards.fema.gov/nri/) and the [CDC Social Vulnerability Index (SVI)](https://www.atsdr.cdc.gov/placeandhealth/svi/index.html).\n",
    "\n",
    "# Preparation\n",
    "\n",
    "1. Create a 'data' folder in the root directory of this repository.\n",
    "1. Inside the 'data' folder, create a 'raw' folder.\n",
    "1. Add and commit a '.gitignore' file to the root directory of this repository that excludes all contents of the 'data' folder.\n",
    "1. Download the county-level NRI and SVI data for the entire United States. Place the data in the 'data/raw' folder.\n",
    "1. In the repository README, provide a brief (1-2 sentence) description of each file in the 'data' folder and a link to the original source of the data.\n",
    "\n",
    "Task 1 - NRI Data Cleaning\n",
    "\n",
    "1. Import the NRI data. Ensure that the [FIPS code](https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code) variable ('STCOFIPS') is correctly identified as a string / character variable. Otherwise, the leading zeros will be removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nyager\\Desktop\\YagerMLCode\\MachineLearning_Fall24\\homework\n"
     ]
    }
   ],
   "source": [
    "# Import Packages Test: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Current WD:\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OID_                int64\n",
      "NRI_ID             object\n",
      "STATE              object\n",
      "STATEABBRV         object\n",
      "STATEFIPS           int64\n",
      "                   ...   \n",
      "WNTW_ALR_NPCTL    float64\n",
      "WNTW_RISKV        float64\n",
      "WNTW_RISKS        float64\n",
      "WNTW_RISKR         object\n",
      "NRI_VER            object\n",
      "Length: 465, dtype: object\n",
      "yes Nolan, it's a string\n"
     ]
    }
   ],
   "source": [
    "# Loading NRI data and specifying STCOFIPS as a string column\n",
    "nri_data = pd.read_csv(\"data/raw/NRI_Data_By_County.csv\", dtype={'STCOFIPS': str})\n",
    "\n",
    "# Here, we check if the State-County FIPS code (STCOFIPS) was loaded correctly\n",
    "print(nri_data.dtypes)  # Verify if 'STCOFIPS' is treated as an object (string)\n",
    "second_value = nri_data['STCOFIPS'].iloc[1]\n",
    "if isinstance(second_value, str):\n",
    "    print(\"yes Nolan, it's a string\")\n",
    "else:\n",
    "    print(\"no Nolan, it isn't a string\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Subset the NRI data to include only the 5-digit state/county FIPS code and all colums ending with '\\_AFREQ' and '\\_RISKR'. Each of these columns represents a different hazard type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  STCOFIPS  AVLN_AFREQ      AVLN_RISKR  CFLD_AFREQ      CFLD_RISKR  \\\n",
      "0    01001         NaN  Not Applicable         NaN  Not Applicable   \n",
      "1    01003         NaN  Not Applicable    3.684142  Relatively Low   \n",
      "2    01005         NaN  Not Applicable         NaN  Not Applicable   \n",
      "3    01007         NaN  Not Applicable         NaN  Not Applicable   \n",
      "4    01009         NaN  Not Applicable         NaN  Not Applicable   \n",
      "\n",
      "   CWAV_AFREQ CWAV_RISKR  DRGT_AFREQ           DRGT_RISKR  ERQK_AFREQ  ...  \\\n",
      "0         0.0  No Rating   25.969774       Relatively Low    0.000431  ...   \n",
      "1         0.0  No Rating   12.353442  Relatively Moderate    0.000338  ...   \n",
      "2         0.0  No Rating   43.956953       Relatively Low    0.000227  ...   \n",
      "3         0.0  No Rating   28.894501             Very Low    0.000790  ...   \n",
      "4         0.0  No Rating   28.152598       Relatively Low    0.000817  ...   \n",
      "\n",
      "  TRND_AFREQ           TRND_RISKR TSUN_AFREQ         TSUN_RISKR VLCN_AFREQ  \\\n",
      "0   0.480184  Relatively Moderate        NaN     Not Applicable        NaN   \n",
      "1   0.953140  Relatively Moderate        NaN  Insufficient Data        NaN   \n",
      "2   0.739018  Relatively Moderate        NaN     Not Applicable        NaN   \n",
      "3   0.586160  Relatively Moderate        NaN     Not Applicable        NaN   \n",
      "4   0.710332  Relatively Moderate        NaN     Not Applicable        NaN   \n",
      "\n",
      "       VLCN_RISKR WFIR_AFREQ           WFIR_RISKR WNTW_AFREQ      WNTW_RISKR  \n",
      "0  Not Applicable   0.000035             Very Low   0.433437        Very Low  \n",
      "1  Not Applicable   0.002229  Relatively Moderate   0.182759  Relatively Low  \n",
      "2  Not Applicable   0.000038             Very Low   0.185759        Very Low  \n",
      "3  Not Applicable   0.000040             Very Low   0.743034        Very Low  \n",
      "4  Not Applicable   0.000035             Very Low   0.866873        Very Low  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# Subsetting: \n",
    "columns_to_keep = ['STCOFIPS'] + [col for col in nri_data.columns if col.endswith('_AFREQ') or col.endswith('_RISKR')]\n",
    "nri_subset = nri_data[columns_to_keep]\n",
    "\n",
    "# Looking at first 5 rows to check: \n",
    "print(nri_subset.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Create a table / dataframe that, for each hazard type, shows the number of missing values in the '\\_AFREQ' and '\\_RISKR' columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hazard_Type  Missing_Values\n",
      "0   AVLN_AFREQ            3023\n",
      "1   AVLN_RISKR               0\n",
      "2   CFLD_AFREQ            2646\n",
      "3   CFLD_RISKR               0\n",
      "4   CWAV_AFREQ               0\n",
      "5   CWAV_RISKR               0\n",
      "6   DRGT_AFREQ               7\n",
      "7   DRGT_RISKR               0\n",
      "8   ERQK_AFREQ               0\n",
      "9   ERQK_RISKR               0\n",
      "10  HAIL_AFREQ               7\n",
      "11  HAIL_RISKR               0\n",
      "12  HWAV_AFREQ               0\n",
      "13  HWAV_RISKR               0\n",
      "14  HRCN_AFREQ             918\n",
      "15  HRCN_RISKR               0\n",
      "16  ISTM_AFREQ             229\n",
      "17  ISTM_RISKR               0\n",
      "18  LNDS_AFREQ              40\n",
      "19  LNDS_RISKR               0\n",
      "20  LTNG_AFREQ             123\n",
      "21  LTNG_RISKR               0\n",
      "22  RFLD_AFREQ               0\n",
      "23  RFLD_RISKR               0\n",
      "24  SWND_AFREQ               7\n",
      "25  SWND_RISKR               0\n",
      "26  TRND_AFREQ               7\n",
      "27  TRND_RISKR               0\n",
      "28  TSUN_AFREQ            3103\n",
      "29  TSUN_RISKR               0\n",
      "30  VLCN_AFREQ            3125\n",
      "31  VLCN_RISKR               0\n",
      "32  WFIR_AFREQ              88\n",
      "33  WFIR_RISKR               0\n",
      "34  WNTW_AFREQ               0\n",
      "35  WNTW_RISKR               0\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe:  \n",
    "afreq_riskr_columns = [col for col in nri_subset.columns if col.endswith('_AFREQ') or col.endswith('_RISKR')]\n",
    "missing_values_nri = nri_subset[afreq_riskr_columns].isna().sum()\n",
    "\n",
    "# Table: \n",
    "summary_table_missing = pd.DataFrame({\n",
    "    'Hazard_Type': missing_values_nri.index, \n",
    "    'Missing_Values': missing_values_nri.values\n",
    "})\n",
    "\n",
    "# Print Table: \n",
    "print(summary_table_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a new column in the original data table indicating whether or not 'AVLN_AFREQ' is missing or observed. Show the cross-tabulation of the 'AVLN_AFREQ' missingness and 'AVLN_RISKR' columns (including missing values). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVLN_RISKR          Not Applicable  Relatively High  Relatively Low  \\\n",
      "AVLN_AFREQ_Missing                                                    \n",
      "False                            0               15              52   \n",
      "True                          3023                0               0   \n",
      "\n",
      "AVLN_RISKR          Relatively Moderate  Very High  Very Low  \n",
      "AVLN_AFREQ_Missing                                            \n",
      "False                                33          9        99  \n",
      "True                                  0          0         0  \n"
     ]
    }
   ],
   "source": [
    "# Create a new column \n",
    "nri_data['AVLN_AFREQ_Missing'] = nri_data['AVLN_AFREQ'].isna()\n",
    "\n",
    "# Cross-tab set up\n",
    "cross_tab = pd.crosstab(nri_data['AVLN_AFREQ_Missing'], nri_data['AVLN_RISKR'], dropna=False)\n",
    "\n",
    "# Printing\n",
    "print(cross_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Assuming that a risk that is \"not applicable\" to a county has an annualized frequency of 0, impute the relevant missing values in the '\\_AFREQ' columns with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVLN_AFREQ    0\n",
      "CFLD_AFREQ    0\n",
      "CWAV_AFREQ    0\n",
      "DRGT_AFREQ    0\n",
      "ERQK_AFREQ    0\n",
      "HAIL_AFREQ    0\n",
      "HWAV_AFREQ    0\n",
      "HRCN_AFREQ    0\n",
      "ISTM_AFREQ    0\n",
      "LNDS_AFREQ    0\n",
      "LTNG_AFREQ    0\n",
      "RFLD_AFREQ    0\n",
      "SWND_AFREQ    0\n",
      "TRND_AFREQ    0\n",
      "TSUN_AFREQ    0\n",
      "VLCN_AFREQ    0\n",
      "WFIR_AFREQ    0\n",
      "WNTW_AFREQ    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Selecting _AFREQ columns \n",
    "afreq_columns = [col for col in nri_data.columns if col.endswith('_AFREQ')]\n",
    "\n",
    "# Filling missing values with 0s \n",
    "nri_data[afreq_columns] = nri_data[afreq_columns].fillna(0)\n",
    "\n",
    "# Printing to see if it worked\n",
    "print(nri_data[afreq_columns].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 - SVI Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the SVI data. Ensure that the FIPS code is correctly identified as a string / character variable. Otherwise, the leading zeros will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading SVI data and telling python FIPS is a string \n",
    "svi_data = pd.read_csv(\"data/raw/SVI_2022_Data_By_County.csv\", dtype={'FIPS': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Subset the SVI data to include only the following columns:\n",
    "`ST, STATE, ST_ABBR, STCNTY, COUNTY, FIPS, LOCATION, AREA_SQMI, E_TOTPOP, EP_POV150, EP_UNEMP, EP_HBURD, EP_NOHSDP, EP_UNINSUR, EP_AGE65, EP_AGE17, EP_DISABL, EP_SNGPNT, EP_LIMENG, EP_MINRTY, EP_MUNIT, EP_MOBILE, EP_CROWD, EP_NOVEH, EP_GROUPQ, EP_NOINT, EP_AFAM, EP_HISP, EP_ASIAN, EP_AIAN, EP_NHPI, EP_TWOMORE, EP_OTHERRACE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting SVI data\n",
    "columns_to_keep = [\n",
    "    'ST', 'STATE', 'ST_ABBR', 'STCNTY', 'COUNTY', 'FIPS', 'LOCATION', 'AREA_SQMI',\n",
    "    'E_TOTPOP', 'EP_POV150', 'EP_UNEMP', 'EP_HBURD', 'EP_NOHSDP', 'EP_UNINSUR', \n",
    "    'EP_AGE65', 'EP_AGE17', 'EP_DISABL', 'EP_SNGPNT', 'EP_LIMENG', 'EP_MINRTY', \n",
    "    'EP_MUNIT', 'EP_MOBILE', 'EP_CROWD', 'EP_NOVEH', 'EP_GROUPQ', 'EP_NOINT', \n",
    "    'EP_AFAM', 'EP_HISP', 'EP_ASIAN', 'EP_AIAN', 'EP_NHPI', 'EP_TWOMORE', 'EP_OTHERRACE'\n",
    "]\n",
    "\n",
    "# Creating the subset: \n",
    "svi_subset = svi_data[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a table / dataframe that shows the number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Columns  Missing_Values_SVI\n",
      "0             ST                   0\n",
      "1          STATE                   0\n",
      "2        ST_ABBR                   0\n",
      "3         STCNTY                   0\n",
      "4         COUNTY                   0\n",
      "5           FIPS                   0\n",
      "6       LOCATION                   0\n",
      "7      AREA_SQMI                   0\n",
      "8       E_TOTPOP                   0\n",
      "9      EP_POV150                   0\n",
      "10      EP_UNEMP                   0\n",
      "11      EP_HBURD                   0\n",
      "12     EP_NOHSDP                   0\n",
      "13    EP_UNINSUR                   0\n",
      "14      EP_AGE65                   0\n",
      "15      EP_AGE17                   0\n",
      "16     EP_DISABL                   0\n",
      "17     EP_SNGPNT                   0\n",
      "18     EP_LIMENG                   0\n",
      "19     EP_MINRTY                   0\n",
      "20      EP_MUNIT                   0\n",
      "21     EP_MOBILE                   0\n",
      "22      EP_CROWD                   0\n",
      "23      EP_NOVEH                   0\n",
      "24     EP_GROUPQ                   0\n",
      "25      EP_NOINT                   0\n",
      "26       EP_AFAM                   0\n",
      "27       EP_HISP                   0\n",
      "28      EP_ASIAN                   0\n",
      "29       EP_AIAN                   0\n",
      "30       EP_NHPI                   0\n",
      "31    EP_TWOMORE                   0\n",
      "32  EP_OTHERRACE                   0\n"
     ]
    }
   ],
   "source": [
    "# Seeing which values are missing by each column in a dataframe: \n",
    "missing_values_svi = svi_subset.isna().sum().reset_index()\n",
    "missing_values_svi.columns = ['Columns', 'Missing_Values_SVI'] \n",
    "\n",
    "# Printing\n",
    "print(missing_values_svi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3 - Data Merging\n",
    "1a. Identify any FIPS codes that are present in the NRI data but not in the SVI data and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIPS codes in NRI but not in SVI: {'72039', '72057', '72015', '72111', '72031', '09015', '72025', '72103', '72139', '72079', '69100', '72107', '72054', '72071', '72087', '72059', '72091', '09003', '72061', '72089', '72129', '72035', '72125', '72053', '72121', '72109', '72041', '72063', '66010', '72073', '72007', '78030', '72029', '72049', '72097', '72019', '72101', '78010', '69120', '72119', '72095', '72105', '72047', '09007', '72123', '72021', '72037', '72115', '72023', '09011', '72067', '72085', '72135', '72133', '72017', '72001', '72027', '09013', '72083', '72151', '09001', '72043', '72051', '72131', '72069', '72127', '09009', '72099', '72141', '72033', '72009', '72147', '78020', '72117', '72145', '72137', '72005', '72093', '72045', '72077', '60020', '72075', '72011', '72081', '72003', '69110', '60050', '72065', '72013', '72149', '72143', '60010', '72055', '72113', '09005', '72153'}\n",
      "FIPS codes in SVI but not in NRI: {'09140', '09120', '09190', '09130', '09150', '09170', '09110', '09180', '09160'}\n"
     ]
    }
   ],
   "source": [
    "# Extracting both FIPS codes as sets: \n",
    "nri_fips = set(nri_subset['STCOFIPS'])\n",
    "svi_fips = set(svi_subset['FIPS'])\n",
    "\n",
    "# FIPS in NRI, but not in SVI data: \n",
    "fips_in_nri_not_svi = nri_fips - svi_fips\n",
    "\n",
    "# Vice versa: FIPS in SVI, but not in NRI: \n",
    "fips_in_svi_not_nri = svi_fips - nri_fips\n",
    "\n",
    "# Printing both: \n",
    "print(f\"FIPS codes in NRI but not in SVI: {fips_in_nri_not_svi}\")\n",
    "print(f\"FIPS codes in SVI but not in NRI: {fips_in_svi_not_nri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b. Describe any discrepancies and possible causes? \n",
    "Answer: There are quite a lot more FIPS codes in the NRI that are not in the SVI than FIPS codes that are in the SVI data, but not in the NRI data. I know from studying Congress that congressional boundaries change occasionally, perhaps this also happens with counties. I suppose it's possible that this data was collected in slightly different years, and in between the collection periods of each some county level boundaries changed. It's also possible that there is simply missing data due to collection issues, such as if there is no data available. Best case scenario, this was missing at random, worst case scenario, there is a systematic reason as to why it is missing. For instance, perhaps the data is missing in the SVI data specifically for counties that lack proper funding, which may be due to having residents with a lower economic status. \n",
    "\n",
    "1c. What do these discrepancies if any, mean for interpreting results based on the merged dataset moving forward?\n",
    "Answer: as I mentioned above, if the missing data is missing NOT at random, that is, there is a systematic reason as to why it is missing, this could lead to biases in our future analyses. For example, above I noted that it's possible SVI data are missing for counties with lower socioeconomic status, which would lead to us having fewer counties with these characteristics. These types of counties would then be less represented in our analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Merge the NRI and SVI data on the FIPS code. Use an outer join to keep all counties in the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FIPS  AVLN_AFREQ      AVLN_RISKR  CFLD_AFREQ      CFLD_RISKR  CWAV_AFREQ  \\\n",
      "0  01001         NaN  Not Applicable         NaN  Not Applicable         0.0   \n",
      "1  01003         NaN  Not Applicable    3.684142  Relatively Low         0.0   \n",
      "2  01005         NaN  Not Applicable         NaN  Not Applicable         0.0   \n",
      "3  01007         NaN  Not Applicable         NaN  Not Applicable         0.0   \n",
      "4  01009         NaN  Not Applicable         NaN  Not Applicable         0.0   \n",
      "\n",
      "  CWAV_RISKR  DRGT_AFREQ           DRGT_RISKR  ERQK_AFREQ  ... EP_NOVEH  \\\n",
      "0  No Rating   25.969774       Relatively Low    0.000431  ...      4.0   \n",
      "1  No Rating   12.353442  Relatively Moderate    0.000338  ...      2.3   \n",
      "2  No Rating   43.956953       Relatively Low    0.000227  ...     11.7   \n",
      "3  No Rating   28.894501             Very Low    0.000790  ...      7.5   \n",
      "4  No Rating   28.152598       Relatively Low    0.000817  ...      4.8   \n",
      "\n",
      "   EP_GROUPQ EP_NOINT  EP_AFAM EP_HISP  EP_ASIAN EP_AIAN  EP_NHPI EP_TWOMORE  \\\n",
      "0        0.9     10.9     19.6     3.2       1.1     0.1      0.0        3.3   \n",
      "1        1.5     10.9      8.3     4.8       0.9     0.2      0.0        3.1   \n",
      "2       12.0     31.8     46.9     4.8       0.5     0.3      0.0        1.8   \n",
      "3        6.4     20.2     20.7     2.9       0.3     0.1      0.0        1.7   \n",
      "4        1.0     16.9      1.2     9.7       0.2     0.1      0.2        2.8   \n",
      "\n",
      "   EP_OTHERRACE  \n",
      "0           0.2  \n",
      "1           0.4  \n",
      "2           1.2  \n",
      "3           0.1  \n",
      "4           0.1  \n",
      "\n",
      "[5 rows x 69 columns]\n"
     ]
    }
   ],
   "source": [
    "# Renaming STCOFIPS in NRI to FIPS (simpler, aligned with SVI data)\n",
    "nri_subset = nri_subset.rename(columns={'STCOFIPS': 'FIPS'})\n",
    "\n",
    "# Perform outer join on the FIPS code \n",
    "nri_svi_merged = pd.merge(nri_subset, svi_subset, on='FIPS', how='outer')\n",
    "\n",
    "# Printing it to check: \n",
    "print(nri_svi_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a table / dataframe that shows the number of missing values in each column of the merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Column  Missing_Values_Merged\n",
      "0           FIPS                      0\n",
      "1     AVLN_AFREQ                   3032\n",
      "2     AVLN_RISKR                      9\n",
      "3     CFLD_AFREQ                   2655\n",
      "4     CFLD_RISKR                      9\n",
      "..           ...                    ...\n",
      "64      EP_ASIAN                     96\n",
      "65       EP_AIAN                     96\n",
      "66       EP_NHPI                     96\n",
      "67    EP_TWOMORE                     96\n",
      "68  EP_OTHERRACE                     96\n",
      "\n",
      "[69 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe showing the number of missing values for each column\n",
    "missing_values_merged = nri_svi_merged.isna().sum().reset_index()\n",
    "missing_values_merged.columns = ['Column', 'Missing_Values_Merged']\n",
    "\n",
    "# Previewing it: \n",
    "print(missing_values_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4 - Data Analysis\n",
    "\n",
    "1. For each numerical variable in the merged dataset, plot a histogram showing the distribution of values.\n",
    "(Hint: write a function to make the histogram for a single variable, then use a loop or apply function to make the histograms for all numerical variables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF HOMEWORK 2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
